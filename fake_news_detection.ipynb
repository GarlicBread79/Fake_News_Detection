{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jdorn\\OneDrive\\Documents\\Fake_News_Project\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\jdorn\\OneDrive\\Documents\\Fake_News_Project\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Fake News Detection System\n",
    "==================================\n",
    "This project implements a robust Fake News Detection system that leverages both traditional machine learning techniques and state-of-the-art deep learning models. \n",
    "It utilizes:\n",
    "\n",
    "- Baseline Model: TF-IDF vectorization paired with Logistic Regression to establish a solid baseline for text classification.\n",
    "- Advanced Model: A fine-tuned BERT-based model to capture deeper contextual nuances in the text for more accurate predictions.\n",
    "The system includes data preprocessing, model training, rigorous evaluation, and real-time predictions on new articles.\n",
    "\n",
    "Author: Joshua Dornfeldt\n",
    "Date: 11/05/2024\n",
    "\"\"\"\n",
    "\n",
    "# Import necessary libraries\n",
    "import numpy as np  # Numerical operations\n",
    "import pandas as pd  # Data manipulation and analysis\n",
    "import matplotlib.pyplot as plt  # Data visualization\n",
    "import seaborn as sns  # Statistical data visualization\n",
    "import tensorflow as tf  # Deep learning framework\n",
    "import torch  # PyTorch framework for deep learning\n",
    "import joblib  # Model persistence\n",
    "import requests  # Handling HTTP requests\n",
    "from sklearn.model_selection import train_test_split  # Train-test splitting\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # TF-IDF text vectorization\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic Regression model\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    ")  # Performance evaluation metrics\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification  # BERT-based NLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset successfully loaded! Shape: (99, 3)\n"
     ]
    }
   ],
   "source": [
    "# Load the Dataset\n",
    "def load_dataset(data_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads the dataset from the specified CSV file path.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): The absolute or relative path to the dataset CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A pandas DataFrame containing the dataset.\n",
    "    \n",
    "    Raises:\n",
    "        FileNotFoundError: If the specified file does not exist.\n",
    "        pd.errors.EmptyDataError: If the file is empty.\n",
    "    \"\"\"\n",
    "    print(\"Loading dataset...\")\n",
    "\n",
    "    try:\n",
    "        df = pd.read_csv(data_path)  # Loads data into a DataFrame\n",
    "        print(f\"Dataset successfully loaded! Shape: {df.shape}\")  # Logs the dataset size\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file at {data_path} was not found.\")\n",
    "        raise\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Error: The file at {data_path} is empty.\")\n",
    "        raise\n",
    "\n",
    "# Defines dataset path \n",
    "DATA_PATH = \"C:/Users/jdorn/OneDrive/Documents/Fake_News_Project/archive/Testing_dataset/testingSet/processed_data.csv\"\n",
    "\n",
    "# Load the dataset\n",
    "df = load_dataset(DATA_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset successfully loaded! Shape: (99, 3)\n",
      "\n",
      "Class Distribution (Real News vs Fake News):\n",
      "Label\n",
      "1    50\n",
      "0    49\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Import the Necessary Library\n",
    "import pandas as pd\n",
    "\n",
    "# Loads Dataset and Analyzes Class Distribution\n",
    "def load_and_analyze_dataset(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads the dataset from a CSV file and displays class distribution.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the dataset CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded dataset as a pandas DataFrame.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the dataset file is not found.\n",
    "        pd.errors.EmptyDataError: If the dataset file is empty.\n",
    "        pd.errors.ParserError: If the CSV file cannot be parsed correctly.\n",
    "    \"\"\"\n",
    "    print(\"Loading dataset...\")\n",
    "\n",
    "    try:\n",
    "        # Loads the dataset into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Displays dataset statistics\n",
    "        print(f\"Dataset successfully loaded! Shape: {df.shape}\")  \n",
    "        print(\"\\nClass Distribution (Real News vs Fake News):\")\n",
    "        print(df['Label'].value_counts())  # Counts the occurrences of each label\n",
    "\n",
    "        return df\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file at {file_path} was not found.\")\n",
    "        raise\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Error: The file at {file_path} is empty.\")\n",
    "        raise\n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"Error: The file at {file_path} could not be parsed.\")\n",
    "        raise\n",
    "\n",
    "# Defines the file path of the dataset \n",
    "DATA_FILE_PATH = \"C:/Users/jdorn/OneDrive/Documents/Fake_News_Project/archive/Testing_dataset/testingSet/processed_data.csv\"\n",
    "\n",
    "# Loads dataset and displays the label distribution\n",
    "df = load_and_analyze_dataset(DATA_FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Dataset Overview ---\n",
      "\n",
      "Dataset Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 99 entries, 0 to 98\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   Title    99 non-null     object\n",
      " 1   Article  99 non-null     object\n",
      " 2   Label    99 non-null     int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 2.4+ KB\n",
      "\n",
      "First Few Records:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                                               Title  \\\n",
       " 0  Germany and France signal willingness to delay...   \n",
       " 1  Ramnit Computer Worm Compromises 45K Facebook ...   \n",
       " 2  5.5 Hours, No CCTVs, No Showering of Cash: SC ...   \n",
       " 3  Is Aspartame Responsible for â€˜An Epidemic of M...   \n",
       " 4  â€˜Breaking Badâ€™ Star Bryan Cranston Diagnosed W...   \n",
       " \n",
       "                                              Article  Label  \n",
       " 0  Germany and France have signalled their willin...      0  \n",
       " 1  A computer worm that has traditionally targete...      0  \n",
       " 2  \\nNEW DELHI â€” The Supreme Court on Thursday se...      0  \n",
       " 3  I have spent several days lecturing at the WOR...      1  \n",
       " 4  HOLLYWOOD, California â€“ \\r\\n\\r\\nIn a tragic tw...      1  ,\n",
       " None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displays the Dataset Summary and First Few Rows of the Dataset\n",
    "def summarize_dataset(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Displays basic information and the first few rows of the dataset.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset to be summarized.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (DataFrame head, None)\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Dataset Overview ---\")\n",
    "    \n",
    "    # Displays basic dataset information\n",
    "    print(\"\\nDataset Info:\")\n",
    "    df.info()\n",
    "\n",
    "    # Displays the first few rows of the dataset\n",
    "    print(\"\\nFirst Few Records:\")\n",
    "    return df.head(), None  # Returning None explicitly for df.info(), as it prints the output but does not return a value\n",
    "\n",
    "# Summarizes the dataset\n",
    "summarize_dataset(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitting the dataset into training and testing sets...\n",
      "Training Set Size: 79 samples\n",
      "Testing Set Size: 20 samples\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# Data Preprocessing - Splitting the Data\n",
    "# =============================\n",
    "\n",
    "def split_data(df: pd.DataFrame, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and testing sets.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataset containing the articles and labels.\n",
    "        test_size (float): Proportion of the dataset to include in the test split. Default is 0.2 (20%).\n",
    "        random_state (int): Random seed for reproducibility. Default is 42.\n",
    "\n",
    "    Returns:\n",
    "        tuple: X_train, X_test, y_train, y_test (split training and test data)\n",
    "    \"\"\"\n",
    "    print(\"\\nSplitting the dataset into training and testing sets...\")\n",
    "\n",
    "    # Extract features (news articles) and labels (real/fake classification)\n",
    "    X = df['Article']  # Feature: News text\n",
    "    y = df['Label']    # Target: Classification label (Real = 0, Fake = 1)\n",
    "\n",
    "    # Performs the train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "\n",
    "    print(f\"Training Set Size: {X_train.shape[0]} samples\")\n",
    "    print(f\"Testing Set Size: {X_test.shape[0]} samples\")\n",
    "\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Executes the function to split the data\n",
    "X_train, X_test, y_train, y_test = split_data(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying TF-IDF vectorization and training Logistic Regression...\n",
      "Logistic Regression model training complete.\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# TF-IDF Vectorization & Logistic Regression Model \n",
    "# ============================================\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def train_logistic_regression(X_train, X_test, y_train):\n",
    "    \"\"\"\n",
    "    Applies TF-IDF vectorization and trains a Logistic Regression model \n",
    "    to classify fake vs. real news articles.\n",
    "\n",
    "    Args:\n",
    "        X_train (pd.Series): Training set of news articles.\n",
    "        X_test (pd.Series): Testing set of news articles.\n",
    "        y_train (pd.Series): Training set labels (0 = Real News, 1 = Fake News).\n",
    "\n",
    "    Returns:\n",
    "        tuple: \n",
    "            logistic_model (LogisticRegression) - Trained logistic regression model.\n",
    "            X_train_tfidf (sparse matrix) - Transformed TF-IDF feature set for training.\n",
    "            X_test_tfidf (sparse matrix) - Transformed TF-IDF feature set for testing.\n",
    "            y_pred_baseline (np.array) - Predicted labels for the test set.\n",
    "            y_prob_baseline (np.array) - Predicted probabilities for the test set.\n",
    "    \"\"\"\n",
    "    print(\"\\nApplying TF-IDF vectorization and training Logistic Regression...\")\n",
    "\n",
    "    # TF-IDF Vectorization (Includes unigrams, bigrams, and trigrams for better context)\n",
    "    tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,3), stop_words=\"english\", max_features=5000)\n",
    "    \n",
    "    # Transforms the text data into TF-IDF features\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "    # Trains the Logistic Regression model\n",
    "    logistic_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    logistic_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    # Predict labels and probabilities on the test set\n",
    "    y_pred_baseline = logistic_model.predict(X_test_tfidf)\n",
    "    y_prob_baseline = logistic_model.predict_proba(X_test_tfidf)[:, 1]\n",
    "\n",
    "    print(\"Logistic Regression model training complete.\")\n",
    "    \n",
    "    return logistic_model, X_train_tfidf, X_test_tfidf, y_pred_baseline, y_prob_baseline\n",
    "\n",
    "# Executes the function to train the model\n",
    "logistic_model, X_train_tfidf, X_test_tfidf, y_pred_baseline, y_prob_baseline = train_logistic_regression(X_train, X_test, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Logistic Regression Model Performance...\n",
      "\n",
      "Baseline Model - Logistic Regression\n",
      "Accuracy: 0.85\n",
      "ROC-AUC: 0.88\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.80      0.84        10\n",
      "           1       0.82      0.90      0.86        10\n",
      "\n",
      "    accuracy                           0.85        20\n",
      "   macro avg       0.85      0.85      0.85        20\n",
      "weighted avg       0.85      0.85      0.85        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Evaluate Logistic Regression Model Performance\n",
    "# ============================================\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "\n",
    "def evaluate_logistic_model(y_test, y_pred, y_prob):\n",
    "    \"\"\"\n",
    "    Evaluates the performance of the Logistic Regression model using accuracy, ROC-AUC score,\n",
    "    and a detailed classification report.\n",
    "\n",
    "    Args:\n",
    "        y_test (np.array or pd.Series): True labels for the test set.\n",
    "        y_pred (np.array): Predicted labels from the logistic model.\n",
    "        y_prob (np.array): Predicted probabilities for the positive class (fake news).\n",
    "\n",
    "    Returns:\n",
    "        dict: Contains accuracy and ROC-AUC scores for logging purposes.\n",
    "    \"\"\"\n",
    "    print(\"\\nEvaluating Logistic Regression Model Performance...\")\n",
    "    \n",
    "    # Computes the evaluation metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "    # Prints the evaluation results\n",
    "    print(\"\\nBaseline Model - Logistic Regression\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"ROC-AUC: {roc_auc:.2f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "    # Returns the results as a dictionary\n",
    "    return {\"accuracy\": accuracy, \"roc_auc\": roc_auc}\n",
    "\n",
    "# Executes the evaluation\n",
    "logistic_metrics = evaluate_logistic_model(y_test, y_pred_baseline, y_prob_baseline)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding test dataset for BERT model...\n",
      "Test data encoded and moved to cpu\n"
     ]
    }
   ],
   "source": [
    "## ============================================\n",
    "# Load BERT Tokenizer & Encode Test Data\n",
    "# ============================================\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "\n",
    "# Loads a pre-trained BERT tokenizer\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def encode_texts(texts, tokenizer, max_len=256):\n",
    "    \"\"\"\n",
    "    Encodes a list of text samples into tokenized BERT inputs with padding and truncation.\n",
    "\n",
    "    Args:\n",
    "        texts (list or pd.Series): List of text samples to encode.\n",
    "        tokenizer (BertTokenizer): Preloaded BERT tokenizer for processing text.\n",
    "        max_len (int): Maximum sequence length for padding/truncation.\n",
    "\n",
    "    Returns:\n",
    "        dict: Tokenized inputs containing input_ids and attention_mask as PyTorch tensors.\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        list(texts),\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\" \n",
    "    )\n",
    "\n",
    "# Encodes the test dataset\n",
    "print(\"Encoding test dataset for BERT model...\")\n",
    "X_test_encoded = encode_texts(X_test, bert_tokenizer)\n",
    "\n",
    "# Ensure computations are performed on the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Moves the encoded input tensors to the selected device\n",
    "input_ids = X_test_encoded[\"input_ids\"].to(device)\n",
    "attention_mask = X_test_encoded[\"attention_mask\"].to(device)\n",
    "\n",
    "print(f\"Test data encoded and moved to {device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer and model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding training and test data...\n",
      "Creating PyTorch datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jdorn\\OneDrive\\Documents\\Fake_News_Project\\venv\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BERT fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jdorn\\AppData\\Local\\Temp\\ipykernel_18912\\3378742198.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 06:56, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Roc Auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.698388</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.696749</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.610000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.694642</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693316</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.691647</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.689526</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jdorn\\AppData\\Local\\Temp\\ipykernel_18912\\3378742198.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\jdorn\\AppData\\Local\\Temp\\ipykernel_18912\\3378742198.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\jdorn\\AppData\\Local\\Temp\\ipykernel_18912\\3378742198.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\jdorn\\AppData\\Local\\Temp\\ipykernel_18912\\3378742198.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\jdorn\\AppData\\Local\\Temp\\ipykernel_18912\\3378742198.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\jdorn\\AppData\\Local\\Temp\\ipykernel_18912\\3378742198.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving fine-tuned BERT model...\n",
      "Reloading fine-tuned BERT model for inference...\n",
      "BERT model is ready for predictions!\n"
     ]
    }
   ],
   "source": [
    "# ============================================\n",
    "# Load & Fine-Tune BERT Model for Fake News Detection\n",
    "# ============================================\n",
    "\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --------------------------------------------\n",
    "# Step 1: Loads the Pretrained BERT Model & Tokenizer\n",
    "# --------------------------------------------\n",
    "print(\"Loading BERT tokenizer and model...\")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# --------------------------------------------\n",
    "# Step 2: Encodes the Text Data for Training\n",
    "# --------------------------------------------\n",
    "def encode_texts(texts, tokenizer, max_len=256):\n",
    "    \"\"\"\n",
    "    Tokenizes and encodes input text into tensors suitable for BERT.\n",
    "    \"\"\"\n",
    "    return tokenizer(\n",
    "        list(texts),\n",
    "        max_length=max_len,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"  \n",
    "    )\n",
    "\n",
    "# Ensures that X_train, X_test, y_train, and y_test are defined from the dataset.\n",
    "print(\"Encoding training and test data...\")\n",
    "train_encodings = encode_texts(X_train, bert_tokenizer)\n",
    "test_encodings = encode_texts(X_test, bert_tokenizer)\n",
    "\n",
    "# --------------------------------------------\n",
    "# Step 3: Converts the Encoded Data to PyTorch Dataset\n",
    "# --------------------------------------------\n",
    "class FakeNewsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "print(\"Creating PyTorch datasets...\")\n",
    "train_data = FakeNewsDataset(train_encodings, y_train.tolist())\n",
    "test_data = FakeNewsDataset(test_encodings, y_test.tolist())\n",
    "\n",
    "# --------------------------------------------\n",
    "# Step 4: Fine-Tune BERT Model with Improved Hyperparameters and Evaluation Metrics\n",
    "# --------------------------------------------\n",
    "\n",
    "# Defines a compute_metrics function to evaluate accuracy and ROC-AUC during training\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    logits = pred.predictions\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    \n",
    "    # Computes the probabilities for the positive class using softmax\n",
    "    probs = F.softmax(torch.tensor(logits), dim=1)[:, 1].numpy()\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    roc_auc = roc_auc_score(labels, probs)\n",
    "    return {\"accuracy\": accuracy, \"roc_auc\": roc_auc}\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=6,            \n",
    "    per_device_train_batch_size=8, \n",
    "    evaluation_strategy=\"epoch\",   \n",
    "    learning_rate=2e-5,            \n",
    "    warmup_steps=500,              \n",
    "    weight_decay=0.01,             # Uses penalties on large weights to help prevent the model from overfitting\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=bert_model, \n",
    "    args=training_args, \n",
    "    train_dataset=train_data, \n",
    "    eval_dataset=test_data,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Starting BERT fine-tuning...\")\n",
    "trainer.train()\n",
    "\n",
    "# --------------------------------------------\n",
    "# Step 5: Save Fine-Tuned Model\n",
    "# --------------------------------------------\n",
    "print(\"Saving fine-tuned BERT model...\")\n",
    "# Disables safe serialization to avoid file access issues\n",
    "bert_model.save_pretrained(\"./bert_finetuned\", safe_serialization=False)\n",
    "bert_tokenizer.save_pretrained(\"./bert_finetuned\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# Step 6: Reload the Fine-Tuned Model for Predictions\n",
    "# --------------------------------------------\n",
    "print(\"Reloading fine-tuned BERT model for inference...\")\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\"./bert_finetuned\")\n",
    "\n",
    "# --------------------------------------------\n",
    "# Step 7: Make Predictions with Fine-Tuned BERT\n",
    "# --------------------------------------------\n",
    "def predict_bert(text):\n",
    "    \"\"\"\n",
    "    Predicts whether a given article is real or fake news using fine-tuned BERT.\n",
    "    \"\"\"\n",
    "    encoded_input = encode_texts([text], bert_tokenizer)\n",
    "    with torch.no_grad():  \n",
    "        logits = bert_model(**encoded_input).logits\n",
    "    pred = torch.argmax(logits, dim=1).item()\n",
    "    return \"Fake News\" if pred == 1 else \"Real News\"\n",
    "\n",
    "print(\"BERT model is ready for predictions!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Starting BERT fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jdorn\\OneDrive\\Documents\\Fake_News_Project\\venv\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\jdorn\\AppData\\Local\\Temp\\ipykernel_18912\\3378742198.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 07:35, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.091600</td>\n",
       "      <td>0.634249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.085600</td>\n",
       "      <td>0.654184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.081400</td>\n",
       "      <td>0.671641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.068000</td>\n",
       "      <td>0.691827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.063000</td>\n",
       "      <td>0.720491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.047700</td>\n",
       "      <td>0.758093</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jdorn\\AppData\\Local\\Temp\\ipykernel_18912\\3378742198.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\jdorn\\AppData\\Local\\Temp\\ipykernel_18912\\3378742198.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\jdorn\\AppData\\Local\\Temp\\ipykernel_18912\\3378742198.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\jdorn\\AppData\\Local\\Temp\\ipykernel_18912\\3378742198.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\jdorn\\AppData\\Local\\Temp\\ipykernel_18912\\3378742198.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
      "C:\\Users\\jdorn\\AppData\\Local\\Temp\\ipykernel_18912\\3378742198.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Model is fine-tuned and ready for evaluation.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Fine-Tuning BERT for Fake News Detection\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 1: Moves Model to GPU \n",
    "# ------------------------------------------------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Ensure the model is on the correct device\n",
    "bert_model.to(device)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 2: Defines Training Arguments for BERT Fine-Tuning\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",             # Directory to save model checkpoints\n",
    "    num_train_epochs=6,                 # Increased epochs for improved learning\n",
    "    per_device_train_batch_size=8,      \n",
    "    per_device_eval_batch_size=8,       \n",
    "    warmup_steps=500,                   \n",
    "    weight_decay=0.01,                   # Uses penalties on large weights to help prevent the model from overfitting\n",
    "    logging_dir=\"./logs\",                \n",
    "    logging_steps=10,                   \n",
    "    evaluation_strategy=\"epoch\",         # Evaluate the model after each epoch\n",
    "    save_strategy=\"epoch\",               # Save checkpoints after every epoch\n",
    "    load_best_model_at_end=True          \n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 3: Create Trainer Instance\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=bert_model,       # Fine-tuned BERT model\n",
    "    args=training_args,     # Training configurations\n",
    "    train_dataset=train_data,  # Training dataset\n",
    "    eval_dataset=test_data     # Evaluation dataset\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 4: Train the Model\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"Starting BERT fine-tuning...\")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"Training complete. Model is fine-tuned and ready for evaluation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Running inference on test data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jdorn\\AppData\\Local\\Temp\\ipykernel_18912\\3154277584.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(X_test_encoded[\"input_ids\"]).to(device)\n",
      "C:\\Users\\jdorn\\AppData\\Local\\Temp\\ipykernel_18912\\3154277584.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask = torch.tensor(X_test_encoded[\"attention_mask\"]).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference complete. Predictions ready for evaluation.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# BERT Model Inference - Fake News Detection\n",
    "# ============================================================\n",
    "\n",
    "import torch\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 1: Move Model and Test Data to GPU\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Converts the test inputs to PyTorch tensors and move them to the correct device\n",
    "input_ids = torch.tensor(X_test_encoded[\"input_ids\"]).to(device)\n",
    "attention_mask = torch.tensor(X_test_encoded[\"attention_mask\"]).to(device)\n",
    "\n",
    "# Ensures that the model is on the correct device and set to evaluation mode\n",
    "bert_model.to(device)\n",
    "bert_model.eval()  # Disables dropout layers and improves inference reliability\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 2: Perform Inference Using Fine-Tuned BERT\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"Running inference on test data...\")\n",
    "\n",
    "with torch.no_grad():  \n",
    "    outputs = bert_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits  # Extracts raw model predictions\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 3: Convert Model Outputs to Class Labels\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# Convert logits (raw scores) into class labels (0 = Real News, 1 = Fake News)\n",
    "y_pred_oracle_labels = torch.argmax(logits, axis=1).cpu().numpy()  # Moves data to CPU for processing\n",
    "\n",
    "print(\"Inference complete. Predictions ready for evaluation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting logits to probabilities...\n",
      "Conversion complete. Proceeding with evaluation metrics.\n",
      "\n",
      " Oracle Model - Fine-Tuned BERT Performance\n",
      "Accuracy: 0.65\n",
      "ROC-AUC: 0.84\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.60      0.63        10\n",
      "           1       0.64      0.70      0.67        10\n",
      "\n",
      "    accuracy                           0.65        20\n",
      "   macro avg       0.65      0.65      0.65        20\n",
      "weighted avg       0.65      0.65      0.65        20\n",
      "\n",
      "Evaluation complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jdorn\\AppData\\Local\\Temp\\ipykernel_18912\\2346250104.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_pred_oracle_probs = F.softmax(torch.tensor(logits), dim=1).cpu().numpy()\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# BERT Model Evaluation - Fake News Detection\n",
    "# ============================================================\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 1: Convert Model Outputs to Probabilities\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"Converting logits to probabilities...\")\n",
    "\n",
    "# Applies softmax to logits to obtain class probabilities\n",
    "y_pred_oracle_probs = F.softmax(torch.tensor(logits), dim=1).cpu().numpy()\n",
    "\n",
    "print(\"Conversion complete. Proceeding with evaluation metrics.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 2: Evaluate BERT Model Performance\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"\\n Oracle Model - Fine-Tuned BERT Performance\")\n",
    "\n",
    "# Computes the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred_oracle_labels)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Computes the ROC-AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_pred_oracle_probs[:, 1])\n",
    "print(f\"ROC-AUC: {roc_auc:.2f}\")\n",
    "\n",
    "# Displays the detailed classification report\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_oracle_labels))\n",
    "\n",
    "print(\"Evaluation complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating model accuracy and ROC-AUC scores...\n",
      "Model evaluation complete. Results stored in 'model_results'.\n",
      "Displaying model comparison results...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>ROC-AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>85.0</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BERT</td>\n",
       "      <td>65.0</td>\n",
       "      <td>84.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy  ROC-AUC\n",
       "0  Logistic Regression      85.0     88.0\n",
       "1                 BERT      65.0     84.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model comparison table displayed successfully.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# Model Performance Comparison - Logistic Regression vs BERT\n",
    "# ============================================================\n",
    "\n",
    "import torch.nn.functional as F  # Import softmax for probability conversion\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import IPython.display as display\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 1: Compute Model Evaluation Metrics\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"Calculating model accuracy and ROC-AUC scores...\")\n",
    "\n",
    "# Create a DataFrame to store results for both models\n",
    "model_results = pd.DataFrame({\n",
    "    \"Model\": [\"Logistic Regression\", \"BERT\"],\n",
    "    \n",
    "    # Computes accuracy for both models\n",
    "    \"Accuracy\": [\n",
    "        accuracy_score(y_test, y_pred_baseline) * 100,  # Logistic Regression\n",
    "        accuracy_score(y_test, y_pred_oracle_labels) * 100  # BERT\n",
    "    ],\n",
    "    \n",
    "    # Computes ROC-AUC score for both models\n",
    "    \"ROC-AUC\": [\n",
    "        roc_auc_score(y_test, y_prob_baseline) * 100,  # Logistic Regression\n",
    "        roc_auc_score(y_test, F.softmax(logits, dim=1)[:, 1].cpu().numpy()) * 100  # BERT model with Softmax applied\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Model evaluation complete. Results stored in 'model_results'.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 2: Display Results in Jupyter Notebook\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"Displaying model comparison results...\")\n",
    "\n",
    "# Use IPython display to render the DataFrame nicely in Jupyter Notebook\n",
    "display.display(model_results)\n",
    "\n",
    "print(\"Model comparison table displayed successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating model performance comparison plots...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jdorn\\AppData\\Local\\Temp\\ipykernel_18912\\3425166068.py:22: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=\"Model\", y=\"Accuracy\", data=model_results, palette=\"viridis\")\n",
      "C:\\Users\\jdorn\\AppData\\Local\\Temp\\ipykernel_18912\\3425166068.py:35: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(x=\"Model\", y=\"ROC-AUC\", data=model_results, palette=\"coolwarm\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualization complete: Accuracy and ROC-AUC comparison displayed.\n"
     ]
    }
   ],
   "source": [
    "# ====================================================================\n",
    "# Model Performance Visualization - Accuracy & ROC-AUC Comparison\n",
    "# ====================================================================\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 1: Set Up the Plot\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"Generating model performance comparison plots...\")\n",
    "\n",
    "# Define a figure size for readability\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 2: Accuracy Comparison Bar Chart\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "plt.subplot(1, 2, 1)  # Defines a first subplot (1 row, 2 columns, first plot)\n",
    "sns.barplot(x=\"Model\", y=\"Accuracy\", data=model_results, palette=\"viridis\")\n",
    "\n",
    "# Customizes a plot appearance\n",
    "plt.title(\"Accuracy Comparison\", fontsize=14, fontweight=\"bold\")\n",
    "plt.ylim(0, 100)  # Sets a consistent scale\n",
    "plt.xlabel(\"\")  # Remove the x-axis label for cleaner visualization\n",
    "plt.ylabel(\"Accuracy (%)\", fontsize=12)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 3: ROC-AUC Comparison Bar Chart\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "plt.subplot(1, 2, 2)  # Defines a second subplot (1 row, 2 columns, second plot)\n",
    "sns.barplot(x=\"Model\", y=\"ROC-AUC\", data=model_results, palette=\"coolwarm\")\n",
    "\n",
    "# Customizes plot appearance\n",
    "plt.title(\"ROC-AUC Score Comparison\", fontsize=14, fontweight=\"bold\")\n",
    "plt.ylim(0, 100)  # Keeps the scale consistent\n",
    "plt.xlabel(\"\")  # Removes the x-axis label\n",
    "plt.ylabel(\"ROC-AUC (%)\", fontsize=12)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 4: Final Adjustments and Display\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "plt.tight_layout()  # Optimizes spacing\n",
    "plt.show()  # Displays the plots\n",
    "\n",
    "print(\"Visualization complete: Accuracy and ROC-AUC comparison displayed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-trained models and vectorizer...\n",
      "WARNING:tensorflow:From c:\\Users\\jdorn\\OneDrive\\Documents\\Fake_News_Project\\venv\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models successfully loaded.\n",
      "Preparing test articles for evaluation...\n",
      " 2 test articles prepared.\n",
      "Prediction functions are ready.\n",
      "\n",
      "Evaluating model predictions...\n",
      "\n",
      "-------------------------------------------------\n",
      " **Article:** NASA announces new mission to Mars set for 2030, focusing on human exploration.\n",
      " **Actual Label:** Real News\n",
      " Logistic Regression Prediction: Fake News\n",
      " BERT Prediction: Real News\n",
      "-------------------------------------------------\n",
      " **Article:** Breaking: Scientists confirm the Earth is flat, sparking worldwide debate.\n",
      " **Actual Label:** Fake News\n",
      " Logistic Regression Prediction: Fake News\n",
      " BERT Prediction: Real News\n",
      "\n",
      " Model evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "# Fake News Detection - Model Evaluation on Real Articles\n",
    "# =============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 1: Load Pre-Trained Models & Vectorizer\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"Loading pre-trained models and vectorizer...\")\n",
    "\n",
    "# Loads the TF-IDF vectorizer and also trained Logistic Regression model\n",
    "tfidf_vectorizer = joblib.load(\"tfidf_vectorizer.pkl\")\n",
    "logistic_model = joblib.load(\"logistic_regression.pkl\")\n",
    "\n",
    "# Loads the BERT model and tokenizer for classification\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "print(\"Models successfully loaded.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 2: Define Real & Fake News Test Samples\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"Preparing test articles for evaluation...\")\n",
    "\n",
    "# Sample test cases with actual labels (0 = Real, 1 = Fake) to test if the program is working\n",
    "test_articles = [\n",
    "    {\"text\": \"NASA announces new mission to Mars set for 2030, focusing on human exploration.\", \"label\": 0},  # Real \n",
    "    {\"text\": \"Breaking: Scientists confirm the Earth is flat, sparking worldwide debate.\", \"label\": 1},  # Fake\n",
    "]\n",
    "\n",
    "print(f\" {len(test_articles)} test articles prepared.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 3: Define Prediction Functions for Both Models\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "def predict_logistic(text):\n",
    "    \"\"\"\n",
    "    Predicts whether a given article is real or fake using the Logistic Regression model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The news article content.\n",
    "\n",
    "    Returns:\n",
    "        str: \"Fake News\" if classified as fake, else \"Real News\".\n",
    "    \"\"\"\n",
    "    text_tfidf = tfidf_vectorizer.transform([text])  # Converts the text to TF-IDF features\n",
    "    pred = logistic_model.predict(text_tfidf)[0]  # Predicts a label (0 = Real, 1 = Fake)\n",
    "    return \"Fake News\" if pred == 1 else \"Real News\"\n",
    "\n",
    "def predict_bert(text):\n",
    "    \"\"\"\n",
    "    Predicts whether a given article is real or fake using the fine-tuned BERT model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The news article content.\n",
    "\n",
    "    Returns:\n",
    "        str: \"Fake News\" if classified as fake, else \"Real News\".\n",
    "    \"\"\"\n",
    "    # Tokenizes the input text for BERT\n",
    "    inputs = bert_tokenizer(\n",
    "        text, return_tensors=\"tf\", max_length=512, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    # Gets the model predictions \n",
    "    logits = bert_model(inputs[\"input_ids\"], inputs[\"attention_mask\"]).logits\n",
    "    pred = tf.argmax(logits, axis=1).numpy()[0]  # Convert logits to a predicted label (0 or 1)\n",
    "    return \"Fake News\" if pred == 1 else \"Real News\"\n",
    "\n",
    "print(\"Prediction functions are ready.\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Step 4: Run Predictions & Compare Results\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "print(\"\\nEvaluating model predictions...\\n\")\n",
    "\n",
    "for article in test_articles:\n",
    "    # Get an actual label as human-readable text\n",
    "    true_label = \"Fake News\" if article[\"label\"] == 1 else \"Real News\"\n",
    "\n",
    "    # Predict using both models\n",
    "    logistic_pred = predict_logistic(article[\"text\"])\n",
    "    bert_pred = predict_bert(article[\"text\"])\n",
    "\n",
    "    # Displays the results\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print(f\" **Article:** {article['text']}\")\n",
    "    print(f\" **Actual Label:** {true_label}\")\n",
    "    print(f\" Logistic Regression Prediction: {logistic_pred}\")\n",
    "    print(f\" BERT Prediction: {bert_pred}\")\n",
    "\n",
    "print(\"\\n Model evaluation complete.\")\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
